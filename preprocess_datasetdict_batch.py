import re
import unicodedata
import torch
import numpy as np
from dataclasses import dataclass
from typing import Any, Dict, List, Union, Optional, Tuple
from transformers import WhisperProcessor
from datasets import DatasetDict, Audio, concatenate_datasets, load_dataset, disable_caching
from loggerinterface import get_logger
logger = get_logger(__name__)
# -------------------------------------------
# 0) Processor ì¤€ë¹„
# -------------------------------------------
MODEL_ID = "openai/whisper-tiny"   # í•„ìš”ì— ë§ê²Œ ë³€ê²½ (tiny/base/small...)
LANG     = "ko"                    # ì–¸ì–´
TASK     = "transcribe"            # í˜¹ì€ "translate"
processor: WhisperProcessor = WhisperProcessor.from_pretrained(MODEL_ID, language=LANG, task=TASK)
TARGET_SR = processor.feature_extractor.sampling_rate  # ë³´í†µ 16000
# -------------------------------------------
# 1) Remove column
# -------------------------------------------
def remove_columns_from_datasetdict(common_voice: DatasetDict, protected=("audio", "sentence")) -> Tuple[Optional[DatasetDict], str]:
    """
    í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ì„ ì œê±°í•´ ë°ì´í„° í¬ê¸°ë¥¼ ì¤„ì´ê³ , ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆê°, ëª¨ë¸ ì…ë ¥ íŒŒì´í”„ë¼ì¸ì„ ë‹¨ìˆœí™”
    Args:
    - ë‹¤ìš´ë¡œë“œ ë°›ì€ DatasetDictë¥¼ ì…ë ¥
    Returns:
    - DatasetDict ë¦¬í„´ (í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ì´ ì œê±°ëœ)
    """
    try:        
        # train ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ ë‚´ì„ì„ setì˜ í˜•íƒœë¡œ í™•ë³´
        present = set(common_voice["train"].column_names)
        # present ì…‹ì—ì„œ í•˜ë‚˜ì”© êº¼ëƒ, drop_candidatesì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        cols_to_drop = [c for c in present if c not in protected]
        logger.info(f"ğŸ§¹ Removed columns: {cols_to_drop}")
        # DatasetDict.remove_columns() ë©”ì„œë“œë¡œ ì¼ê´„ ì œê±° ë° ìƒˆë¡œìš´ DatasetDict ê°ì²´ë¥¼ ë°˜í™˜
        # ë©”ëª¨ë ˆì´ íš¨ìœ¨ì ì´ë©° Arrow ê¸°ë°˜ìœ¼ë¡œ ì¦‰ì‹œ ë°˜ì˜
        return common_voice.remove_columns(cols_to_drop), ""
    
    except AttributeError:
        return None, "âŒ Error: ì…ë ¥ì´ DatasetDict ê°ì²´ê°€ ì•„ë‹™ë‹ˆë‹¤."
    except ValueError as e:
        return None, f"âŒ ì»¬ëŸ¼ ì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    except Exception as e:
        return None, f"âš ï¸ Unexpected Error: {e}"
    

# -------------------------------------------
# 2) Filtering
# -------------------------------------------
# ===== 0) (í•„ìš”ì‹œ) ì˜¤ë””ì˜¤ ì»¬ëŸ¼ ìºìŠ¤íŒ… =====
# ì´ë¯¸ Audio íƒ€ì…ì´ë©´ ìƒëµ ê°€ëŠ¥
def ensure_audio_cast(dsd: DatasetDict, audio_col="audio", sr=16000) -> DatasetDict:
    return dsd.cast_column(audio_col, Audio(sampling_rate=sr))

# ===== 1) ê¸¸ì´(ms) ê³„ì‚° (í•œ ë²ˆë§Œ) =====
def add_duration_ms(dsd: DatasetDict, audio_col="audio") -> DatasetDict:
    def _dur(b):
        a = b[audio_col]
        return {"duration_ms": int(len(a["array"]) * 1000 / a["sampling_rate"])}
    return DatasetDict({k: v.map(_dur) for k, v in dsd.items()})

# ===== 2) ì˜¤ë””ì˜¤ ê¸¸ì´ í•„í„° =====
def filter_by_duration(dsd: DatasetDict, min_ms=500, max_ms=30000) -> DatasetDict:
    def _ok(ex):
        d = ex.get("duration_ms")
        return (d is not None) and (min_ms <= d <= max_ms)
    return DatasetDict({k: v.filter(_ok) for k, v in dsd.items()})

# ===== 3) ë¬¸ì¥ ì •ê·œí™” & ë¬¸ì œ í…ìŠ¤íŠ¸ í•„í„° =====
_DISALLOWED_TOKENS = {
    "[noise]", "<noise>", "(noise)", "[music]", "<music>", "(music)",
    "[laughter]", "<laughter>", "(laughter)", "<unk>", "[unk]"
}

# í•œê¸€/ì˜ë¬¸/ìˆ«ì ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ëŠ”ì§€ (ë‚´ìš© ì¡´ì¬ íŒë‹¨)
_CONTENT_RE = re.compile(r"[A-Za-z0-9\uAC00-\uD7A3]")

def normalize_sentence(s: str) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFC", s)
    s = s.replace("â€¦", "...")         # ì˜ˆ: í†µì¼
    s = re.sub(r"\s+", " ", s)        # ì—°ì† ê³µë°± ì •ë¦¬
    return s.strip()

def has_disallowed_tokens(s: str) -> bool:
    s_low = s.lower()
    return any(tok in s_low for tok in _DISALLOWED_TOKENS)

def looks_like_text(s: str) -> bool:
    # ë‚´ìš©(ë¬¸ì/ìˆ«ì/í•œê¸€)ì´ í•˜ë‚˜ë¼ë„ ìˆì–´ì•¼ í•¨
    return bool(_CONTENT_RE.search(s))

def add_sentence_norm_and_filter(
    dsd: DatasetDict,
    sent_col="sentence",
    min_chars=1,
    max_chars=200
) -> DatasetDict:
    # 3-1) ì •ê·œí™”ëœ ë¬¸ì¥ ì¶”ê°€
    def _norm(b):
        s = normalize_sentence(b.get(sent_col, ""))
        return {"sentence_norm": s}
    dsd2 = DatasetDict({k: v.map(_norm) for k, v in dsd.items()})

    # 3-2) ë¬¸ì œ ë¬¸ì¥ ì œê±°
    def _ok(ex):
        s = ex["sentence_norm"]
        L = len(s)
        if not (min_chars <= L <= max_chars):
            return False
        if has_disallowed_tokens(s):
            return False
        if not looks_like_text(s):
            return False
        return True

    return DatasetDict({k: v.filter(_ok) for k, v in dsd2.items()})

# ===== 4) í•œ ë²ˆì— ì ìš©í•˜ëŠ” í—¬í¼ =====
def apply_audio_text_filters(
    dsd: DatasetDict,
    audio_col="audio",
    sent_col="sentence",
    sr=16000,
    min_ms=500,
    max_ms=30000,
    min_chars=1,
    max_chars=200
) -> DatasetDict:    
    dsd = add_duration_ms(dsd, audio_col=audio_col)
    dsd = filter_by_duration(dsd, min_ms=min_ms, max_ms=max_ms)
    dsd = add_sentence_norm_and_filter(dsd, sent_col=sent_col,
                                       min_chars=min_chars, max_chars=max_chars)
    return dsd


# -------------------------------------------
# 1) DataCollator (íŒ¨ë”© + label -100 ë§ˆìŠ¤í‚¹)
#    * transformersì˜ DataCollatorSpeechSeq2SeqWithPaddingì„ ì¨ë„ ë¬´ë°©
# -------------------------------------------
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any                      # WhisperProcessor
    decoder_start_token_id: int         # ë°˜ë“œì‹œ ì£¼ì…!

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # 1) ì˜¤ë””ì˜¤ -> input_features
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # 2) í…ìŠ¤íŠ¸ ë¼ë²¨ -> padding & -100 ë§ˆìŠ¤í‚¹
        def to_list_ids(x):
            return x.tolist() if isinstance(x, torch.Tensor) else x

        label_features = [{"input_ids": to_list_ids(f["labels"])} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, padding=True, return_tensors="pt")

        labels = labels_batch["input_ids"]
        attention_mask = labels_batch["attention_mask"]
        labels = labels.masked_fill(attention_mask.eq(0), -100).to(torch.long)

        # 3) decoder_input_ids ìƒì„± (shift-right)
        tok = self.processor.tokenizer
        # padê°€ ì—†ë‹¤ë©´ eosë¡œ ë§ì¶°ë‘ê¸°(ë§ˆì§€ë§‰ ë°©ì–´)
        pad_id = tok.pad_token_id if tok.pad_token_id is not None else tok.eos_token_id
        start_id = self.decoder_start_token_id if self.decoder_start_token_id is not None else pad_id

        labels_for_shift = labels.clone()
        labels_for_shift[labels_for_shift == -100] = pad_id

        # [start_id] + labels[:-1]
        decoder_input_ids = torch.full((labels_for_shift.size(0), 1), start_id, dtype=torch.long)
        decoder_input_ids = torch.cat([decoder_input_ids, labels_for_shift[:, :-1]], dim=1)

        # ë°©ì–´ì  ì²´í¬(ë¬¸ì œ ìˆìœ¼ë©´ ë°”ë¡œ ì‹¤íŒ¨í•´ì„œ ì›ì¸ íŒŒì•… ì‰¬ì›€)
        assert decoder_input_ids is not None and decoder_input_ids.ndim == 2, "decoder_input_ids malformed"
        assert "input_features" in batch and batch["input_features"].ndim == 3, "input_features missing or malformed"
        assert labels is not None and labels.ndim == 2, "labels malformed"

        batch["decoder_input_ids"] = decoder_input_ids
        batch["labels"] = labels
        return batch
# -------------------------------------------
# 2) batched mapìš© ì „ì²˜ë¦¬ í•¨ìˆ˜
#    - ì˜¤ë””ì˜¤ ë¦¬ìŠ¤íŠ¸ â†’ log-mel input_features
#    - ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ â†’ token id ì‹œí€€ìŠ¤ labels
#    - ë°˜í™˜ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(Arrow ìºì‹œ ì•ˆì •ì„± â†‘)
# -------------------------------------------
def prepare_dataset_with_processor(
    batch: Dict[str, List[Any]],
    processor: WhisperProcessor,
    audio_col: str = "audio",
    text_col: str = "sentence",
    feature_dtype: str = "float32",     # "float32" ê¶Œì¥ (ì›í•˜ë©´ "float16")
    max_target_len: int = None,         # í•„ìš” ì‹œ í† í° ê¸¸ì´ ì œí•œ
    truncation: bool = True
) -> Dict[str, List[Any]]:

    # ë¹ˆ ë°°ì¹˜ ë°©ì–´
    if not batch.get(audio_col):
        return {"input_features": [], "labels": []}

    # 1) ì˜¤ë””ì˜¤ ë°°ì—´/ìƒ˜í”Œë ˆì´íŠ¸ ì¶”ì¶œ
    audio_arrays = [a["array"] for a in batch[audio_col]]
    # WhisperProcessorê°€ ìë™ ë¦¬ìƒ˜í”Œë§í•˜ë¯€ë¡œ sampling_rateëŠ” TARGET_SR ì‚¬ìš©
    sampling_rate = TARGET_SR

    # 2) ì˜¤ë””ì˜¤ â†’ log-mel íŠ¹ì§• (ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜)
    inputs = processor(
        audio_arrays,
        sampling_rate=sampling_rate,
        return_tensors=None,              # í…ì„œëŠ” collatorì—ì„œ ì²˜ë¦¬
        # return_attention_mask=True,     # í•„ìš” ì‹œ í™œì„±í™”
    )
    feats = inputs["input_features"]
    if feature_dtype == "float16":
        feats = [f.astype("float16") for f in feats]
    else:
        feats = [f.astype("float32") for f in feats]

    # 3) í…ìŠ¤íŠ¸ â†’ token ids (ë¦¬ìŠ¤íŠ¸)
    # as_target_processor()ë¥¼ ì¨ë„ ë˜ì§€ë§Œ, tokenizer ì§ì ‘ í˜¸ì¶œì´ ëª…í™•í•©ë‹ˆë‹¤.
    tok_kwargs = dict(padding="longest", truncation=truncation, return_tensors=None)
    if max_target_len is not None:
        tok_kwargs["max_length"] = max_target_len

    labels = processor.tokenizer(batch[text_col], **tok_kwargs)["input_ids"]

    return {
        "input_features": feats,    # List[np.ndarray]  (80, T)
        "labels": labels,           # List[List[int]]
        # ë””ë²„ê¹…ìš©ìœ¼ë¡œ ê¸¸ì´ ë³´ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
        # "input_length": [f.shape[-1] for f in feats],
    }

# -------------------------------------------
# 3) DatasetDict ì „ì²´ì— ì „ì²˜ë¦¬ ì ìš©
# -------------------------------------------
def preprocess_datasetdict_batched(
    dsd: DatasetDict,
    processor: WhisperProcessor,
    audio_col: str = "audio",
    text_col: str = "sentence",
    num_proc: int = None,            # CPU ìƒí™©ì— ë§ê²Œ (Noneì´ë©´ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)
    batch_size: int = 32,            # batched=Trueì¼ ë•Œì˜ map batch í¬ê¸°
    remove_others: bool = False,     # Trueë©´ input_features/labelsë§Œ ë‚¨ê¹€
    feature_dtype: str = "float32",
    max_target_len: int = None,
    truncation: bool = True
) -> DatasetDict:

    # Audio ìºìŠ¤íŒ… ë³´ì¥ (ê²½ë¡œ/ì›ì‹œ ë°°ì—´ì—ì„œ ìë™ ë¡œë”© + ë¦¬ìƒ˜í”Œë§)
    dsd = dsd.cast_column(audio_col, Audio(sampling_rate=TARGET_SR))

    """
    Dataset.map(
        function,             # ê° ìƒ˜í”Œ(ë˜ëŠ” ë°°ì¹˜)ì— ì ìš©í•  í•¨ìˆ˜
        batched=False,        # Trueë©´ batch ë‹¨ìœ„ë¡œ, Falseë©´ ê°œë³„ ìƒ˜í”Œ ë‹¨ìœ„
        batch_size=1000,      # batched=Trueì¼ ë•Œ ë°°ì¹˜ í¬ê¸°
        num_proc=None,        # ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        remove_columns=None,  # ì§€ì •í•œ ì»¬ëŸ¼ì„ ì œê±°
        load_from_cache_file=True,  # ì´ì „ ê²°ê³¼ ìºì‹œ ì‚¬ìš©
    )
    """
    dsd_proc = dsd.map(
        lambda b: prepare_dataset_with_processor(
            b, processor,
            audio_col=audio_col, text_col=text_col,
            feature_dtype=feature_dtype,
            max_target_len=max_target_len,
            truncation=truncation
        ),
        batched=True,
        batch_size=batch_size,
        num_proc=num_proc,
        desc="Preparing Whisper features"
    )

    if remove_others:
        keep = {"input_features", "labels"}
        dsd_proc = DatasetDict({
            k: v.remove_columns([c for c in v.column_names if c not in keep])
            for k, v in dsd_proc.items()
        })

    return dsd_proc